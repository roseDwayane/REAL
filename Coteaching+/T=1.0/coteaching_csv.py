
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Co-Teaching on SimSiam EEG features (CSV+PKL)
---------------------------------------------
- Scans CSV trials (30x1024) under base_path.
- Loads per-file feature .pkl arrays generated by eeg_csv_simsiam.py.
- Builds epoch-level dataset and trains two small MLPs with Co-Teaching(+) to split white/black list.
- Writes predictions back to epochs CSV and marks Black_List in info CSV by voting.
"""

import os, re, json, math, pickle
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F

# -----------------------------
# Config (edit these to your paths)
# -----------------------------

base_path = "./demo_data"  # CSV root; same layout as SimSiam script
class_names = ['CTL', 'CM']
class_paths = {'CTL': 'CTL', 'CM': 'CM'}
class_labels = {'CTL': 0, 'CM': 1}

# Must match the SimSiam feature dimension you produced earlier
feature_dim = 128     # from eeg_csv_simsiam.py (eeg_config['dim'])
features_path = "./Features/csvsimsiam_1.5_1.0"  # folder containing <CSV>.pkl

# Training hyper-params
cfg = dict(
    duration=1.5,        # seconds (sampling rate assumed 250 Hz)
    stride=1.0,
    epochs=5,            # set larger for real runs
    batch_size=256,
    lr=5e-3,
    weight_decay=1e-4,
    init_epoch=1,        # warm-up epochs (co-teaching before ++)
    forget_rate=0.2,
    device='cuda' if torch.cuda.is_available() else 'cpu',
    vote_threshold=0.75, # per-file consensus threshold for whitelist
)

# Optional filename prefix filter like L_ / R_ ; leave empty to disable
keyword_prefixes = []     # e.g., ['L_', 'R_']

CSV_FILE_RE = re.compile(r'^[LR]_\d+_\d+_\d+\.csv$', re.IGNORECASE)

def mkdir(p):
    Path(p).mkdir(parents=True, exist_ok=True)


# -----------------------------
# Data scanning / info building
# -----------------------------

def collect_dataset_info_with_black_list(info_file: str):
    df = {'file': [], 'class_name': [], 'label': [], 'n_times': [], 'Black_List': []}
    for cname in class_names:
        root = os.path.join(base_path, class_paths[cname])
        for r, _, files in os.walk(root):
            for f in files:
                if not f.lower().endswith('.csv'):
                    continue
                if keyword_prefixes and f[:2] not in [k[:2] for k in keyword_prefixes]:
                    continue
                # 若不想限制命名規則, 註解下一行
                if not CSV_FILE_RE.match(f):
                    continue
                fp = os.path.join(r, f)
                # CSV 固定 1024 點
                df['file'].append(fp)
                df['class_name'].append(cname)
                df['label'].append(class_labels[cname])
                df['n_times'].append(1024)
                df['Black_List'].append(False)

    out = pd.DataFrame(df)
    out.to_csv(info_file, index=True)
    print(f"[info] wrote {info_file} with {len(out)} rows")
    return out


# -----------------------------
# Build epoch table and feature array
# -----------------------------

def build_epochs_df(info_df: pd.DataFrame, duration=1.5, stride=1.0, fs=250):
    samples = int(duration * fs) + 1
    step = int(stride * fs)
    rows = []
    gid = 0
    for idx, row in info_df.iterrows():
        T = int(row['n_times'])
        n_epochs = max(0, (T - samples) // step + 1)
        if n_epochs == 0:  # 如果窗口比總長還長, 至少切 1 片
            n_epochs = 1
        for eid in range(n_epochs):
            rows.append({
                'global_id': gid,
                'fif_idx': idx,
                'ID': eid,
                'label': int(row['label']),
                'file': row['file']
            })
            gid += 1
    df_ep = pd.DataFrame(rows)
    return df_ep, samples


def load_feature_data_to_array(info_df: pd.DataFrame, feature_dim: int, features_path: str):
    """
    Returns:
      X: torch.FloatTensor [N_total, feature_dim]
      y: torch.LongTensor  [N_total]
      offset: dict[file_basename] -> (start, length)
    """
    x_list, y_list = [], []
    offset = {}
    cur = 0
    for _, row in info_df.iterrows():
        fp = str(row['file']).strip()
        base = os.path.basename(fp)
        pkl_path = os.path.join(features_path, base + ".pkl")
        if not os.path.exists(pkl_path):
            raise FileNotFoundError(f"Missing feature pkl: {pkl_path}")
        with open(pkl_path, "rb") as f:
            arr = pickle.load(f)  # (N_epochs, feature_dim)
        if arr.ndim != 2 or arr.shape[1] != feature_dim:
            raise ValueError(f"Feature shape mismatch for {pkl_path}: {arr.shape}, expected (*,{feature_dim})")
        x_list.append(arr.astype('float32'))
        y_list += [int(row['label'])] * arr.shape[0]
        offset[base] = (cur, arr.shape[0])
        cur += arr.shape[0]
    X = torch.from_numpy(np.concatenate(x_list, axis=0))
    y = torch.from_numpy(np.array(y_list, dtype=np.int64))
    print(f"[features] X={tuple(X.shape)}, y={tuple(y.shape)}")
    return X, y, offset


# -----------------------------
# Datasets
# -----------------------------

class EpochIndexDataset(torch.utils.data.Dataset):
    """
    Access features by a list of epoch indices.
    """
    def __init__(self, indices, X: torch.Tensor, y: torch.Tensor):
        self.ids = np.array(indices, dtype=np.int64)
        self.X = X
        self.y = y
    def __len__(self):
        return len(self.ids)
    def __getitem__(self, i):
        idx = self.ids[i]
        return self.X[idx], self.y[idx]


def build_loader(df_epochs: pd.DataFrame, X: torch.Tensor, y: torch.Tensor,
                 batch_size=256, shuffle=True):
    inds = df_epochs['global_id'].to_numpy()
    ds = EpochIndexDataset(inds, X, y)
    return torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=shuffle,
                                       drop_last=False, num_workers=0, pin_memory=True)


# -----------------------------
# Models & meters
# -----------------------------

class MLPClassifier(nn.Module):
    def __init__(self, in_dim, hidden=64, num_classes=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, num_classes)
        )
    def forward(self, x):
        return self.net(x)

class AccuracyMeter:
    def __init__(self):
        self.n = 0
        self.correct = 0
        self.proba = []
        self.true = []
    def update(self, logits, y):
        probs = torch.softmax(logits, dim=1).detach().cpu().numpy()
        pred = probs.argmax(axis=1)
        y = y.detach().cpu().numpy()
        self.correct += (pred == y).sum()
        self.n += len(y)
        self.proba.append(probs)
        self.true.append(y)
    def value(self):
        return (self.correct / max(1, self.n)) if self.n else 0.0
    def get_proba(self):
        return np.concatenate(self.proba, axis=0) if self.proba else np.zeros((0,2), dtype=np.float32)


# -----------------------------
# Co-Teaching losses (simplified)
# -----------------------------

def coteaching_losses(logits1, logits2, y, forget_rate):
    """
    Basic Co-Teaching: each network selects small-loss instances for the other.
    """
    per1 = torch.nn.functional.cross_entropy(logits1, y, reduction='none')
    per2 = torch.nn.functional.cross_entropy(logits2, y, reduction='none')

    remember_rate = 1.0 - forget_rate
    num_remember = max(1, int(remember_rate * len(y)))

    idx1 = torch.argsort(per1)[:num_remember]
    idx2 = torch.argsort(per2)[:num_remember]

    loss1_update = torch.mean(torch.nn.functional.cross_entropy(logits1[idx2], y[idx2], reduction='none'))
    loss2_update = torch.mean(torch.nn.functional.cross_entropy(logits2[idx1], y[idx1], reduction='none'))

    return loss1_update, loss2_update, num_remember


# -----------------------------
# Training & prediction
# -----------------------------

def train_coteaching(X, y, df_train, df_val, epochs=5, batch_size=256,
                     lr=5e-3, weight_decay=1e-4, init_epoch=1,
                     forget_rate=0.2, device='cpu'):
    model1 = MLPClassifier(feature_dim, hidden=64, num_classes=2).to(device)
    model2 = MLPClassifier(feature_dim, hidden=64, num_classes=2).to(device)
    opt1 = torch.optim.SGD(model1.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)
    opt2 = torch.optim.SGD(model2.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)

    tr_loader = build_loader(df_train, X, y, batch_size=batch_size, shuffle=True)
    va_loader = build_loader(df_val, X, y, batch_size=1024, shuffle=False)

    for ep in range(epochs):
        model1.train(); model2.train()
        for xb, yb in tr_loader:
            xb = xb.to(device).float(); yb = yb.to(device)
            logits1 = model1(xb); logits2 = model2(xb)
            if ep < init_epoch:
                # warm-up: standard CE
                loss1 = torch.nn.functional.cross_entropy(logits1, yb)
                loss2 = torch.nn.functional.cross_entropy(logits2, yb)
                opt1.zero_grad(); loss1.backward(); opt1.step()
                opt2.zero_grad(); loss2.backward(); opt2.step()
            else:
                loss1, loss2, _ = coteaching_losses(logits1, logits2, yb, forget_rate)
                opt1.zero_grad(); loss1.backward(); opt1.step()
                opt2.zero_grad(); loss2.backward(); opt2.step()
        # quick val
        model1.eval(); model2.eval()
        m1 = AccuracyMeter(); m2 = AccuracyMeter()
        with torch.no_grad():
            for xb, yb in va_loader:
                xb = xb.to(device).float(); yb = yb.to(device)
                m1.update(model1(xb), yb); m2.update(model2(xb), yb)
        print(f"Epoch {ep+1}/{epochs}  Acc1={m1.value():.4f}  Acc2={m2.value():.4f}")
    return model1, model2


def predict_epochs(model1, model2, X, y, df_epochs, device='cpu'):
    loader = build_loader(df_epochs, X, y, batch_size=1024, shuffle=False)
    m1 = AccuracyMeter(); m2 = AccuracyMeter()
    with torch.no_grad():
        model1.eval(); model2.eval()
        for xb, yb in loader:
            xb = xb.to(device).float(); yb = yb.to(device)
            m1.update(model1(xb), yb); m2.update(model2(xb), yb)
    prob1 = m1.get_proba()
    prob2 = m2.get_proba()
    return prob1, prob2


# -----------------------------
# Voting: build white/black list
# -----------------------------

def vote_white_black(info_df: pd.DataFrame, df_epochs: pd.DataFrame, y_prob: np.ndarray,
                     vote_threshold=0.75):
    """
    Mark a file as white if its epoch predictions agree with its label >= threshold.
    Otherwise mark black.
    """
    df_epochs = df_epochs.copy()
    df_epochs['pred'] = y_prob.argmax(axis=1)
    # aggregate per file index
    rates = {}
    for idx in info_df.index:
        ep_idx = df_epochs[df_epochs['fif_idx'] == idx].index.values
        if len(ep_idx) == 0:
            rates[idx] = 0.0
            continue
        lab = int(info_df.loc[idx, 'label'])
        correct = (df_epochs.loc[ep_idx, 'pred'].to_numpy() == lab).mean()
        rates[idx] = float(correct)

    info_df = info_df.copy()
    info_df['Rate'] = info_df.index.map(lambda i: rates.get(i, 0.0))
    info_df['Black_List'] = info_df['Rate'] < vote_threshold
    return info_df


# -----------------------------
# Orchestration
# -----------------------------

def main():
    device = cfg['device']
    print("Device:", device)
    info_file = "coteach_info.csv"
    epochs_file = "coteach_epochs.csv"

    info_df = collect_dataset_info_with_black_list(info_file)
    df_epochs, samples = build_epochs_df(info_df, duration=cfg['duration'], stride=cfg['stride'])
    df_epochs.to_csv(epochs_file, index=False)
    print(f"[epochs] {len(df_epochs)} rows -> saved {epochs_file}")

    X, y, offset = load_feature_data_to_array(info_df, feature_dim, features_path)

    # for this simple run, treat whitelist as everyone initially
    df_wl = info_df[~info_df['Black_List']]
    df_train = df_epochs[df_epochs['fif_idx'].isin(df_wl.index)]
    df_val = df_train.copy()

    m1, m2 = train_coteaching(X, y, df_train, df_val,
                              epochs=cfg['epochs'],
                              batch_size=cfg['batch_size'],
                              lr=cfg['lr'],
                              weight_decay=cfg['weight_decay'],
                              init_epoch=cfg['init_epoch'],
                              forget_rate=cfg['forget_rate'],
                              device=device)

    # predict on all epochs
    prob1, prob2 = predict_epochs(m1, m2, X, y, df_epochs, device=device)
    # save epoch-level probabilities
    df_ep = df_epochs.copy()
    df_ep['Prob10'] = prob1[:,0]; df_ep['Prob11'] = prob1[:,1]
    df_ep['Prob20'] = prob2[:,0]; df_ep['Prob21'] = prob2[:,1]
    df_ep['Pred1'] = prob1.argmax(axis=1)
    df_ep['Pred2'] = prob2.argmax(axis=1)
    df_ep.to_csv("coteach_epochs_pred.csv", index=False)
    print("[save] coteach_epochs_pred.csv")

    # vote -> white/black
    prob = (prob1 + prob2) / 2.0
    info_voted = vote_white_black(info_df, df_epochs, prob, vote_threshold=cfg['vote_threshold'])
    info_voted.to_csv("coteach_info_voted.csv", index=True)
    print("[save] coteach_info_voted.csv")
    print("Summary:")
    print(info_voted[['class_name','label','Rate','Black_List']].groupby(['class_name','Black_List']).size())

if __name__ == '__main__':
    main()
